{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea75a7d",
   "metadata": {},
   "source": [
    "# First Recsys using FAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5bc6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cr_learn import  ml\n",
    "import corerec.engines.unionizedFilterEngine.fast\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78d2c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/visheshyadav/.cache/crlearn/datasets/ml_1m\n",
      "File type detection: {'/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/users.dat': 'users', '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/ratings.dat': 'ratings', '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/movies.dat': 'movies'}\n",
      "Actual file mapping: {'users': '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/users.dat', 'ratings': '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/ratings.dat', 'movies': '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/movies.dat'}\n",
      "Successfully loaded users data from /Users/visheshyadav/.cache/crlearn/datasets/ml_1m/users.dat\n",
      "Successfully loaded ratings data from /Users/visheshyadav/.cache/crlearn/datasets/ml_1m/ratings.dat\n",
      "Successfully loaded movies data from /Users/visheshyadav/.cache/crlearn/datasets/ml_1m/movies.dat\n",
      "Total ratings: 1000209\n",
      "Unique users: 6040\n",
      "Unique movies: 3706\n"
     ]
    }
   ],
   "source": [
    "# load the dataset - returns a dictionary\n",
    "data = ml.load()\n",
    "ratings_df = data['ratings']  # has user_id, movie_id, rating, timestamp\n",
    "\n",
    "print(f\"Total ratings: {len(ratings_df)}\")\n",
    "print(f\"Unique users: {ratings_df['user_id'].nunique()}\")\n",
    "print(f\"Unique movies: {ratings_df['movie_id'].nunique()}\")\n",
    "\n",
    "# split into train/test - 80/20 split\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# extract arrays for the model\n",
    "train_users = train_df['user_id'].values\n",
    "train_items = train_df['movie_id'].values\n",
    "train_ratings = train_df['rating'].values\n",
    "\n",
    "test_users = test_df['user_id'].values\n",
    "test_items = test_df['movie_id'].values\n",
    "test_ratings = test_df['rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f789580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from corerec.engines.unionizedFilterEngine.fast import FAST\n",
    "\n",
    "model = FAST(\n",
    "    factors=32,       \n",
    "    iterations=10,     \n",
    "    batch_size=256,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Training model...\")\n",
    "model.fit(train_users, train_items, train_ratings)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5dcdcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 recommendations for user 1:\n",
      "1. Movie ID: 2905\n",
      "2. Movie ID: 1198\n",
      "3. Movie ID: 912\n",
      "4. Movie ID: 318\n",
      "5. Movie ID: 2019\n",
      "6. Movie ID: 1148\n",
      "7. Movie ID: 1262\n",
      "8. Movie ID: 910\n",
      "9. Movie ID: 953\n",
      "10. Movie ID: 3469\n"
     ]
    }
   ],
   "source": [
    "# get top 10 recommendations for user 1\n",
    "user_id = 1\n",
    "recommendations = model.recommend(user_id, top_n=10)\n",
    "\n",
    "print(f\"\\nTop 10 recommendations for user {user_id}:\")\n",
    "for i, movie_id in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. Movie ID: {movie_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed7886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.2570\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_pred = []\n",
    "for u, i in zip(test_users[:1000], test_items[:1000]):\n",
    "    try:\n",
    "        pred = model.predict(u, i)\n",
    "        test_pred.append(pred)\n",
    "    except:\n",
    "        test_pred.append(3.0)  \n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(test_ratings[:1000], test_pred))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067f2233",
   "metadata": {},
   "source": [
    "# Deep Learning for Recommendations: DeepFM Tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18fead62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/visheshyadav/.cache/crlearn/datasets/ml_1m\n",
      "File type detection: {'/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/users.dat': 'users', '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/ratings.dat': 'ratings', '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/movies.dat': 'movies'}\n",
      "Actual file mapping: {'users': '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/users.dat', 'ratings': '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/ratings.dat', 'movies': '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/movies.dat'}\n",
      "Successfully loaded users data from /Users/visheshyadav/.cache/crlearn/datasets/ml_1m/users.dat\n",
      "Successfully loaded ratings data from /Users/visheshyadav/.cache/crlearn/datasets/ml_1m/ratings.dat\n",
      "Successfully loaded movies data from /Users/visheshyadav/.cache/crlearn/datasets/ml_1m/movies.dat\n",
      "Dataset size: 1000209 ratings\n"
     ]
    }
   ],
   "source": [
    "from cr_learn import ml_1m\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# load larger dataset\n",
    "data = ml_1m.load()\n",
    "ratings_df = data['ratings']\n",
    "\n",
    "print(f\"Dataset size: {len(ratings_df)} ratings\")\n",
    "\n",
    "# convert ratings to binary (like/dislike) for implicit feedback\n",
    "# ratings >= 4 become 1, others become 0\n",
    "ratings_df['interaction'] = (ratings_df['rating'] >= 4).astype(int)\n",
    "\n",
    "# split data\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_users = train_df['user_id'].values\n",
    "train_items = train_df['movie_id'].values\n",
    "train_interactions = train_df['interaction'].values\n",
    "\n",
    "test_users = test_df['user_id'].values\n",
    "test_items = test_df['movie_id'].values\n",
    "test_interactions = test_df['interaction'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a05669c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized: DeepFM(name='DeepFM', fitted=False)\n"
     ]
    }
   ],
   "source": [
    "from corerec import engines\n",
    "\n",
    "# DeepFM combines FM and deep network\n",
    "model = engines.DeepFM(\n",
    "    embedding_dim=64,           # size of embedding vectors\n",
    "    hidden_layers=[128, 64, 32], # deep network layers\n",
    "    epochs=5,                   # training epochs\n",
    "    batch_size=512,\n",
    "    learning_rate=0.001,\n",
    "    device=\"cpu\"  # use \"cuda\" if you have GPU\n",
    ")\n",
    "\n",
    "print(f\"Model initialized: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cb2e0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DeepFM...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training DeepFM...\")\n",
    "model.fit(train_users, train_items, train_interactions)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d59dc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.7983\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# predict on test set\n",
    "test_scores = []\n",
    "for u, i in zip(test_users[:5000], test_items[:5000]):\n",
    "    try:\n",
    "        score = model.predict(u, i)\n",
    "        test_scores.append(score)\n",
    "    except:\n",
    "        test_scores.append(0.0)\n",
    "\n",
    "# calculate AUC\n",
    "auc = roc_auc_score(test_interactions[:5000], test_scores)\n",
    "print(f\"Test AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b3cfd4",
   "metadata": {},
   "source": [
    "# Sequential Recommendations with SASRec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc3d2850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/visheshyadav/.cache/crlearn/datasets/ml_1m\n",
      "File type detection: {'/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/users.dat': 'users', '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/ratings.dat': 'ratings', '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/movies.dat': 'movies'}\n",
      "Actual file mapping: {'users': '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/users.dat', 'ratings': '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/ratings.dat', 'movies': '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/movies.dat'}\n",
      "Successfully loaded users data from /Users/visheshyadav/.cache/crlearn/datasets/ml_1m/users.dat\n",
      "Successfully loaded ratings data from /Users/visheshyadav/.cache/crlearn/datasets/ml_1m/ratings.dat\n",
      "Successfully loaded movies data from /Users/visheshyadav/.cache/crlearn/datasets/ml_1m/movies.dat\n",
      "Users with >= 5 interactions: 6040\n"
     ]
    }
   ],
   "source": [
    "from cr_learn import ml_1m\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# load data\n",
    "data = ml_1m.load()\n",
    "ratings_df = data['ratings']\n",
    "\n",
    "# sort by timestamp to preserve order\n",
    "ratings_df = ratings_df.sort_values('timestamp')\n",
    "\n",
    "# build user sequences\n",
    "user_sequences = defaultdict(list)\n",
    "for _, row in ratings_df.iterrows():\n",
    "    user_sequences[row['user_id']].append((row['movie_id'], row['timestamp']))\n",
    "\n",
    "# filter users with at least 5 interactions\n",
    "min_interactions = 5\n",
    "filtered_users = {u: seq for u, seq in user_sequences.items() \n",
    "                  if len(seq) >= min_interactions}\n",
    "\n",
    "print(f\"Users with >= {min_interactions} interactions: {len(filtered_users)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ed5c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "# create user and item mappings\n",
    "all_users = sorted(filtered_users.keys())\n",
    "all_items = sorted(set([item for seq in filtered_users.values() \n",
    "                       for item, _ in seq]))\n",
    "\n",
    "user_map = {u: i for i, u in enumerate(all_users)}\n",
    "item_map = {i: idx for idx, i in enumerate(all_items)}\n",
    "\n",
    "# build sparse matrix\n",
    "rows, cols, data = [], [], []\n",
    "for u, seq in filtered_users.items():\n",
    "    for item, _ in seq:\n",
    "        rows.append(user_map[u])\n",
    "        cols.append(item_map[item])\n",
    "        data.append(1.0)\n",
    "\n",
    "interaction_matrix = csr_matrix((data, (rows, cols)), \n",
    "                                shape=(len(all_users), len(all_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee3083e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SASRec model initialized\n"
     ]
    }
   ],
   "source": [
    "from corerec import engines\n",
    "\n",
    "model = engines.SASRec(\n",
    "    hidden_units=64,        # hidden dimension\n",
    "    num_blocks=2,           # transformer blocks\n",
    "    num_heads=2,           # attention heads\n",
    "    num_epochs=1,\n",
    "    batch_size=256,\n",
    "    max_seq_length=50,     # max sequence length\n",
    "    learning_rate=0.0001,\n",
    "    device=\"mps\"\n",
    ")\n",
    "\n",
    "print(\"SASRec model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e38a351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SASRec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/visheshyadav/mambaforge/envs/ml2/lib/python3.12/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "2025-11-30 14:52:26,811 - SASRec_5372153488 - INFO - Created 994169 training instances\n",
      "2025-11-30 14:52:26,811 - INFO - Created 994169 training instances\n",
      "2025-11-30 14:52:33,015 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 0 Loss: 0.6900\n",
      "2025-11-30 14:52:33,015 - INFO - Epoch 1/1 Batch 0 Loss: 0.6900\n",
      "2025-11-30 14:52:48,711 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 100 Loss: 0.6677\n",
      "2025-11-30 14:52:48,711 - INFO - Epoch 1/1 Batch 100 Loss: 0.6677\n",
      "2025-11-30 14:53:03,605 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 200 Loss: 0.6394\n",
      "2025-11-30 14:53:03,605 - INFO - Epoch 1/1 Batch 200 Loss: 0.6394\n",
      "2025-11-30 14:53:18,470 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 300 Loss: 0.6104\n",
      "2025-11-30 14:53:18,470 - INFO - Epoch 1/1 Batch 300 Loss: 0.6104\n",
      "2025-11-30 14:53:34,430 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 400 Loss: 0.5743\n",
      "2025-11-30 14:53:34,430 - INFO - Epoch 1/1 Batch 400 Loss: 0.5743\n",
      "2025-11-30 14:53:49,872 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 500 Loss: 0.5375\n",
      "2025-11-30 14:53:49,872 - INFO - Epoch 1/1 Batch 500 Loss: 0.5375\n",
      "2025-11-30 14:54:04,848 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 600 Loss: 0.5328\n",
      "2025-11-30 14:54:04,848 - INFO - Epoch 1/1 Batch 600 Loss: 0.5328\n",
      "2025-11-30 14:54:19,901 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 700 Loss: 0.5287\n",
      "2025-11-30 14:54:19,901 - INFO - Epoch 1/1 Batch 700 Loss: 0.5287\n",
      "2025-11-30 14:54:34,766 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 800 Loss: 0.5062\n",
      "2025-11-30 14:54:34,766 - INFO - Epoch 1/1 Batch 800 Loss: 0.5062\n",
      "2025-11-30 14:54:50,607 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 900 Loss: 0.5350\n",
      "2025-11-30 14:54:50,607 - INFO - Epoch 1/1 Batch 900 Loss: 0.5350\n",
      "2025-11-30 14:55:05,367 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 1000 Loss: 0.4974\n",
      "2025-11-30 14:55:05,367 - INFO - Epoch 1/1 Batch 1000 Loss: 0.4974\n",
      "2025-11-30 14:55:20,214 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 1100 Loss: 0.4757\n",
      "2025-11-30 14:55:20,214 - INFO - Epoch 1/1 Batch 1100 Loss: 0.4757\n",
      "2025-11-30 14:55:35,006 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 1200 Loss: 0.4329\n",
      "2025-11-30 14:55:35,006 - INFO - Epoch 1/1 Batch 1200 Loss: 0.4329\n",
      "2025-11-30 14:55:50,378 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 1300 Loss: 0.4748\n",
      "2025-11-30 14:55:50,378 - INFO - Epoch 1/1 Batch 1300 Loss: 0.4748\n",
      "2025-11-30 14:56:05,562 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 1400 Loss: 0.4161\n",
      "2025-11-30 14:56:05,562 - INFO - Epoch 1/1 Batch 1400 Loss: 0.4161\n",
      "2025-11-30 14:56:20,391 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 1500 Loss: 0.4577\n",
      "2025-11-30 14:56:20,391 - INFO - Epoch 1/1 Batch 1500 Loss: 0.4577\n",
      "2025-11-30 14:56:35,046 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 1600 Loss: 0.4104\n",
      "2025-11-30 14:56:35,046 - INFO - Epoch 1/1 Batch 1600 Loss: 0.4104\n",
      "2025-11-30 14:56:50,384 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 1700 Loss: 0.4007\n",
      "2025-11-30 14:56:50,384 - INFO - Epoch 1/1 Batch 1700 Loss: 0.4007\n",
      "2025-11-30 14:57:05,122 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 1800 Loss: 0.4117\n",
      "2025-11-30 14:57:05,122 - INFO - Epoch 1/1 Batch 1800 Loss: 0.4117\n",
      "2025-11-30 14:57:19,806 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 1900 Loss: 0.4051\n",
      "2025-11-30 14:57:19,806 - INFO - Epoch 1/1 Batch 1900 Loss: 0.4051\n",
      "2025-11-30 14:57:34,852 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 2000 Loss: 0.3777\n",
      "2025-11-30 14:57:34,852 - INFO - Epoch 1/1 Batch 2000 Loss: 0.3777\n",
      "2025-11-30 14:57:50,078 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 2100 Loss: 0.3842\n",
      "2025-11-30 14:57:50,078 - INFO - Epoch 1/1 Batch 2100 Loss: 0.3842\n",
      "2025-11-30 14:58:04,764 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 2200 Loss: 0.3921\n",
      "2025-11-30 14:58:04,764 - INFO - Epoch 1/1 Batch 2200 Loss: 0.3921\n",
      "2025-11-30 14:58:19,437 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 2300 Loss: 0.3568\n",
      "2025-11-30 14:58:19,437 - INFO - Epoch 1/1 Batch 2300 Loss: 0.3568\n",
      "2025-11-30 14:58:34,614 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 2400 Loss: 0.3537\n",
      "2025-11-30 14:58:34,614 - INFO - Epoch 1/1 Batch 2400 Loss: 0.3537\n",
      "2025-11-30 14:58:49,944 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 2500 Loss: 0.3483\n",
      "2025-11-30 14:58:49,944 - INFO - Epoch 1/1 Batch 2500 Loss: 0.3483\n",
      "2025-11-30 14:59:04,650 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 2600 Loss: 0.3575\n",
      "2025-11-30 14:59:04,650 - INFO - Epoch 1/1 Batch 2600 Loss: 0.3575\n",
      "2025-11-30 14:59:19,840 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 2700 Loss: 0.3176\n",
      "2025-11-30 14:59:19,840 - INFO - Epoch 1/1 Batch 2700 Loss: 0.3176\n",
      "2025-11-30 14:59:34,597 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 2800 Loss: 0.3357\n",
      "2025-11-30 14:59:34,597 - INFO - Epoch 1/1 Batch 2800 Loss: 0.3357\n",
      "2025-11-30 14:59:52,756 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 2900 Loss: 0.3353\n",
      "2025-11-30 14:59:52,756 - INFO - Epoch 1/1 Batch 2900 Loss: 0.3353\n",
      "2025-11-30 15:00:06,970 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 3000 Loss: 0.3282\n",
      "2025-11-30 15:00:06,970 - INFO - Epoch 1/1 Batch 3000 Loss: 0.3282\n",
      "2025-11-30 15:00:20,887 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 3100 Loss: 0.3079\n",
      "2025-11-30 15:00:20,887 - INFO - Epoch 1/1 Batch 3100 Loss: 0.3079\n",
      "2025-11-30 15:00:34,604 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 3200 Loss: 0.3213\n",
      "2025-11-30 15:00:34,604 - INFO - Epoch 1/1 Batch 3200 Loss: 0.3213\n",
      "2025-11-30 15:00:48,490 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 3300 Loss: 0.3078\n",
      "2025-11-30 15:00:48,490 - INFO - Epoch 1/1 Batch 3300 Loss: 0.3078\n",
      "2025-11-30 15:01:03,075 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 3400 Loss: 0.3103\n",
      "2025-11-30 15:01:03,075 - INFO - Epoch 1/1 Batch 3400 Loss: 0.3103\n",
      "2025-11-30 15:01:19,171 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 3500 Loss: 0.2849\n",
      "2025-11-30 15:01:19,171 - INFO - Epoch 1/1 Batch 3500 Loss: 0.2849\n",
      "2025-11-30 15:01:33,803 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 3600 Loss: 0.2641\n",
      "2025-11-30 15:01:33,803 - INFO - Epoch 1/1 Batch 3600 Loss: 0.2641\n",
      "2025-11-30 15:01:48,821 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 3700 Loss: 0.2961\n",
      "2025-11-30 15:01:48,821 - INFO - Epoch 1/1 Batch 3700 Loss: 0.2961\n",
      "2025-11-30 15:02:03,263 - SASRec_5372153488 - INFO - Epoch 1/1 Batch 3800 Loss: 0.2766\n",
      "2025-11-30 15:02:03,263 - INFO - Epoch 1/1 Batch 3800 Loss: 0.2766\n",
      "2025-11-30 15:02:16,632 - SASRec_5372153488 - INFO - Epoch 1/1, Loss: 0.4137\n",
      "2025-11-30 15:02:16,632 - INFO - Epoch 1/1, Loss: 0.4137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# convert item IDs to mapped indices\n",
    "mapped_user_ids = [user_map[u] for u in all_users]\n",
    "mapped_item_ids = [item_map[i] for i in all_items]\n",
    "\n",
    "print(\"Training SASRec...\")\n",
    "model.fit(\n",
    "    user_ids=mapped_user_ids,\n",
    "    item_ids=mapped_item_ids,\n",
    "    interaction_matrix=interaction_matrix\n",
    ")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e5446bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequential recommendations for user 1:\n",
      "1. Item 3418\n",
      "2. Item 3471\n",
      "3. Item 2858\n",
      "4. Item 3421\n",
      "5. Item 3361\n",
      "6. Item 3175\n",
      "7. Item 3481\n",
      "8. Item 2916\n",
      "9. Item 3448\n",
      "10. Item 3527\n"
     ]
    }
   ],
   "source": [
    "# recommend for a user based on their sequence\n",
    "test_user_idx = 0\n",
    "test_user_id = all_users[test_user_idx]\n",
    "\n",
    "recommendations = model.recommend(\n",
    "    user_id=test_user_idx,\n",
    "    top_n=10,\n",
    "    exclude_seen=True\n",
    ")\n",
    "\n",
    "# map back to original item IDs\n",
    "recommended_items = [all_items[idx] for idx in recommendations]\n",
    "\n",
    "print(f\"\\nSequential recommendations for user {test_user_id}:\")\n",
    "for rank, item_id in enumerate(recommended_items, 1):\n",
    "    print(f\"{rank}. Item {item_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b575f9",
   "metadata": {},
   "source": [
    "# Building a Production Recommendation System\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14d0cee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/visheshyadav/.cache/crlearn/datasets/ml_1m\n",
      "File type detection: {'/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/users.dat': 'users', '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/ratings.dat': 'ratings', '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/movies.dat': 'movies'}\n",
      "Actual file mapping: {'users': '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/users.dat', 'ratings': '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/ratings.dat', 'movies': '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/movies.dat'}\n",
      "Successfully loaded users data from /Users/visheshyadav/.cache/crlearn/datasets/ml_1m/users.dat\n",
      "Successfully loaded ratings data from /Users/visheshyadav/.cache/crlearn/datasets/ml_1m/ratings.dat\n",
      "Successfully loaded movies data from /Users/visheshyadav/.cache/crlearn/datasets/ml_1m/movies.dat\n",
      "Train: 700146, Val: 150031, Test: 150032\n"
     ]
    }
   ],
   "source": [
    "from cr_learn import ml_1m\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# load and prepare data\n",
    "data = ml_1m.load()\n",
    "ratings_df = data['ratings']\n",
    "\n",
    "# create train/validation/test splits\n",
    "train_df, temp_df = train_test_split(ratings_df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# extract features\n",
    "train_users = train_df['user_id'].values\n",
    "train_items = train_df['movie_id'].values\n",
    "train_ratings = (train_df['rating'] >= 4).astype(int).values\n",
    "\n",
    "val_users = val_df['user_id'].values\n",
    "val_items = val_df['movie_id'].values\n",
    "val_ratings = (val_df['rating'] >= 4).astype(int).values\n",
    "\n",
    "test_users = test_df['user_id'].values\n",
    "test_items = test_df['movie_id'].values\n",
    "test_ratings = (test_df['rating'] >= 4).astype(int).values\n",
    "\n",
    "print(f\"Train: {len(train_users)}, Val: {len(val_users)}, Test: {len(test_users)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "658d5f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Evaluating on validation set...\n",
      "Validation AUC: 0.7934\n"
     ]
    }
   ],
   "source": [
    "from corerec import engines\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support\n",
    "\n",
    "# initialize model\n",
    "model = engines.DeepFM(\n",
    "    embedding_dim=128,\n",
    "    hidden_layers=[256, 128, 64],\n",
    "    epochs=10,\n",
    "    batch_size=1024,\n",
    "    learning_rate=0.001,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# train on training set\n",
    "print(\"Training model...\")\n",
    "model.fit(train_users, train_items, train_ratings)\n",
    "\n",
    "# validate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_scores = []\n",
    "for u, i in zip(val_users[:10000], val_items[:10000]):\n",
    "    try:\n",
    "        score = model.predict(u, i)\n",
    "        val_scores.append(score)\n",
    "    except:\n",
    "        val_scores.append(0.0)\n",
    "\n",
    "val_auc = roc_auc_score(val_ratings[:10000], val_scores)\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "281e8cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Metrics:\n",
      "  AUC: 0.7954\n",
      "  Precision: 0.7520\n",
      "  Recall: 0.7938\n",
      "  F1: 0.7723\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, users, items, ratings, name=\"Test\"):\n",
    "    \"\"\"Evaluate model with multiple metrics\"\"\"\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    # sample for faster evaluation\n",
    "    sample_size = min(10000, len(users))\n",
    "    indices = np.random.choice(len(users), sample_size, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        u, i, r = users[idx], items[idx], ratings[idx]\n",
    "        try:\n",
    "            pred = model.predict(u, i)\n",
    "            predictions.append(pred)\n",
    "            actuals.append(r)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if len(predictions) == 0:\n",
    "        return {}\n",
    "    \n",
    "    auc = roc_auc_score(actuals, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        actuals, \n",
    "        [1 if p > 0.5 else 0 for p in predictions],\n",
    "        average='binary',\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{name} Metrics:\")\n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1: {f1:.4f}\")\n",
    "    \n",
    "    return {'auc': auc, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "# evaluate on test set\n",
    "test_metrics = evaluate_model(model, test_users, test_items, test_ratings, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d1cc9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to saved_models/production_deepfm.pkl\n",
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model_dir = Path(\"saved_models\")\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_path = model_dir / \"production_deepfm.pkl\"\n",
    "model.save(str(model_path))\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# also save metadata\n",
    "metadata = {\n",
    "    'train_size': len(train_users),\n",
    "    'val_auc': val_auc,\n",
    "    'test_metrics': test_metrics,\n",
    "    'model_type': 'DeepFM'\n",
    "}\n",
    "\n",
    "with open(model_dir / \"metadata.pkl\", 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "# load model later\n",
    "loaded_model = engines.DeepFM.load(str(model_path))\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b56e1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated recommendations for 100 users\n",
      "Average recommendations per user: 20.0\n"
     ]
    }
   ],
   "source": [
    "def batch_recommend(model, user_ids, top_k=10):\n",
    "    \"\"\"Generate recommendations for multiple users efficiently\"\"\"\n",
    "    all_recommendations = {}\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        try:\n",
    "            recs = model.recommend(user_id, top_n=top_k, exclude_seen=True)\n",
    "            all_recommendations[user_id] = recs\n",
    "        except:\n",
    "            all_recommendations[user_id] = []\n",
    "    \n",
    "    return all_recommendations\n",
    "\n",
    "# example: recommend for 100 users\n",
    "sample_users = np.unique(test_users)[:100]\n",
    "batch_recs = batch_recommend(model, sample_users, top_k=20)\n",
    "\n",
    "print(f\"\\nGenerated recommendations for {len(batch_recs)} users\")\n",
    "print(f\"Average recommendations per user: {np.mean([len(r) for r in batch_recs.values()]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b1cff",
   "metadata": {},
   "source": [
    "# Industry-Standard Recommendation System: Complete Pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd503ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/visheshyadav/.cache/crlearn/datasets/ml_1m\n",
      "File type detection: {'/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/users.dat': 'users', '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/ratings.dat': 'ratings', '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/movies.dat': 'movies'}\n",
      "Actual file mapping: {'users': '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/users.dat', 'ratings': '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/ratings.dat', 'movies': '/Users/visheshyadav/.cache/crlearn/datasets/ml_1m/movies.dat'}\n",
      "Successfully loaded users data from /Users/visheshyadav/.cache/crlearn/datasets/ml_1m/users.dat\n",
      "Successfully loaded ratings data from /Users/visheshyadav/.cache/crlearn/datasets/ml_1m/ratings.dat\n",
      "Successfully loaded movies data from /Users/visheshyadav/.cache/crlearn/datasets/ml_1m/movies.dat\n",
      "Training DeepFM...\n",
      "Training DCN...\n",
      "Training GNNRec...\n"
     ]
    }
   ],
   "source": [
    "from cr_learn import ml_1m\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from corerec import engines\n",
    "\n",
    "# load data\n",
    "data = ml_1m.load()\n",
    "ratings_df = data['ratings']\n",
    "ratings_df['interaction'] = (ratings_df['rating'] >= 4).astype(int)\n",
    "\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_users = train_df['user_id'].values\n",
    "train_items = train_df['movie_id'].values\n",
    "train_interactions = train_df['interaction'].values\n",
    "\n",
    "# train multiple models\n",
    "models = {}\n",
    "\n",
    "# Model 1: DeepFM\n",
    "print(\"Training DeepFM...\")\n",
    "models['deepfm'] = engines.DeepFM(\n",
    "    embedding_dim=128, hidden_layers=[256, 128], \n",
    "    epochs=5, batch_size=512, device=\"cpu\"\n",
    ")\n",
    "models['deepfm'].fit(train_users, train_items, train_interactions)\n",
    "\n",
    "# Model 2: DCN (Deep & Cross Network)\n",
    "print(\"Training DCN...\")\n",
    "models['dcn'] = engines.DCN(\n",
    "    embedding_dim=128, num_cross_layers=3, deep_layers=[256, 128],\n",
    "    epochs=5, batch_size=512, device=\"cpu\"\n",
    ")\n",
    "models['dcn'].fit(train_users, train_items, train_interactions)\n",
    "\n",
    "# Model 3: GNN-based (if available)\n",
    "try:\n",
    "    print(\"Training GNNRec...\")\n",
    "    models['gnn'] = engines.GNNRec(\n",
    "        embedding_dim=128, num_gnn_layers=2,\n",
    "        epochs=5, batch_size=512, device=\"cpu\"\n",
    "    )\n",
    "    models['gnn'].fit(train_users, train_items, train_interactions)\n",
    "except:\n",
    "    print(\"GNNRec not available, skipping\")\n",
    "\n",
    "print(f\"Trained {len(models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, user_id, item_id, weights=None):\n",
    "    \"\"\"Weighted ensemble of model predictions\"\"\"\n",
    "    if weights is None:\n",
    "        weights = {name: 1.0 / len(models) for name in models.keys()}\n",
    "    \n",
    "    predictions = []\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            pred = model.predict(user_id, item_id)\n",
    "            predictions.append(pred * weights.get(name, 0.0))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return sum(predictions) if predictions else 0.0\n",
    "\n",
    "def ensemble_recommend(models, user_id, top_k=10, weights=None):\n",
    "    \"\"\"Get ensemble recommendations\"\"\"\n",
    "    # get all candidate items\n",
    "    all_items = np.unique(train_items)\n",
    "    \n",
    "    # score all items\n",
    "    item_scores = []\n",
    "    for item_id in all_items:\n",
    "        score = ensemble_predict(models, user_id, item_id, weights)\n",
    "        item_scores.append((item_id, score))\n",
    "    \n",
    "    # sort and return top-k\n",
    "    item_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [item for item, _ in item_scores[:top_k]]\n",
    "\n",
    "# test ensemble\n",
    "test_user = train_users[0]\n",
    "ensemble_recs = ensemble_recommend(models, test_user, top_k=20)\n",
    "print(f\"Ensemble recommendations for user {test_user}: {ensemble_recs[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corerec.core_rec import GraphTransformerV2, train_model, predict\n",
    "from corerec.cr_utility.dataset import GraphDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# build user-item graph adjacency matrix\n",
    "unique_users = sorted(set(train_users))\n",
    "unique_items = sorted(set(train_items))\n",
    "user_map = {u: i for i, u in enumerate(unique_users)}\n",
    "item_map = {i: idx for idx, i in enumerate(unique_items)}\n",
    "\n",
    "# create bipartite graph: users + items\n",
    "n_nodes = len(unique_users) + len(unique_items)\n",
    "adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "# connect users to items they interacted with\n",
    "for u, i in zip(train_users[:10000], train_items[:10000]):  # sample for speed\n",
    "    u_idx = user_map[u]\n",
    "    i_idx = item_map[i] + len(unique_users)\n",
    "    adj_matrix[u_idx, i_idx] = 1.0\n",
    "    adj_matrix[i_idx, u_idx] = 1.0  # undirected\n",
    "\n",
    "print(f\"Graph created: {n_nodes} nodes, {np.sum(adj_matrix > 0) / 2} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf289a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corerec.engines.contentFilterEngine.tfidf_recommender import TFIDFRecommender\n",
    "\n",
    "# content-based model using movie titles/descriptions\n",
    "if 'movies' in data:\n",
    "    movies_df = data['movies']\n",
    "    # create item content dictionary\n",
    "    item_content = {}\n",
    "    for _, row in movies_df.iterrows():\n",
    "        # combine title and genres as content\n",
    "        content = f\"{row.get('title', '')} {row.get('genres', '')}\"\n",
    "        item_content[row['movie_id']] = content\n",
    "    \n",
    "    # train TF-IDF model\n",
    "    content_model = TFIDFRecommender()\n",
    "    item_ids = list(item_content.keys())\n",
    "    contents = [item_content[iid] for iid in item_ids]\n",
    "    content_model.fit(item_ids, item_content)\n",
    "    \n",
    "    print(\"Content-based model trained\")\n",
    "    \n",
    "    # hybrid recommendation: combine collaborative + content\n",
    "    def hybrid_recommend(collab_models, content_model, user_id, item_content, top_k=10):\n",
    "        # get collaborative recommendations\n",
    "        collab_recs = ensemble_recommend(collab_models, user_id, top_k=top_k*2)\n",
    "        \n",
    "        # get content similarity for user's liked items\n",
    "        user_items = train_items[train_users == user_id][:5]  # recent items\n",
    "        if len(user_items) > 0:\n",
    "            # find items similar to user's preferences\n",
    "            user_pref_text = \" \".join([item_content.get(i, \"\") for i in user_items])\n",
    "            content_recs = content_model.recommend_by_text(user_pref_text, top_n=top_k)\n",
    "            \n",
    "            # combine and deduplicate\n",
    "            all_recs = list(set(collab_recs + content_recs))\n",
    "            return all_recs[:top_k]\n",
    "        else:\n",
    "            return collab_recs[:top_k]\n",
    "    \n",
    "    hybrid_recs = hybrid_recommend(models, content_model, test_user, item_content)\n",
    "    print(f\"Hybrid recommendations: {hybrid_recs[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0c2a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class RecommendationService:\n",
    "    \"\"\"Production recommendation service\"\"\"\n",
    "    \n",
    "    def __init__(self, models, content_model=None):\n",
    "        self.models = models\n",
    "        self.content_model = content_model\n",
    "        self.user_map = {u: i for i, u in enumerate(unique_users)}\n",
    "        self.item_map = {i: idx for idx, i in enumerate(unique_items)}\n",
    "    \n",
    "    def recommend(self, user_id, top_k=10, strategy='ensemble'):\n",
    "        \"\"\"Get recommendations for a user\"\"\"\n",
    "        if strategy == 'ensemble':\n",
    "            return ensemble_recommend(self.models, user_id, top_k)\n",
    "        elif strategy == 'deepfm':\n",
    "            return self.models['deepfm'].recommend(user_id, top_n=top_k)\n",
    "        elif strategy == 'dcn':\n",
    "            return self.models['dcn'].recommend(user_id, top_n=top_k)\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def predict_score(self, user_id, item_id):\n",
    "        \"\"\"Predict interaction score\"\"\"\n",
    "        return ensemble_predict(self.models, user_id, item_id)\n",
    "    \n",
    "    def batch_recommend(self, user_ids, top_k=10):\n",
    "        \"\"\"Batch recommendations\"\"\"\n",
    "        results = {}\n",
    "        for uid in user_ids:\n",
    "            results[uid] = self.recommend(uid, top_k)\n",
    "        return results\n",
    "\n",
    "# create service\n",
    "service = RecommendationService(models)\n",
    "\n",
    "# example API-like usage\n",
    "user_recs = service.recommend(test_user, top_k=20, strategy='ensemble')\n",
    "print(f\"Service recommendations: {user_recs[:10]}\")\n",
    "\n",
    "# batch processing\n",
    "batch_users = np.unique(train_users)[:50]\n",
    "batch_results = service.batch_recommend(batch_users, top_k=15)\n",
    "print(f\"Processed {len(batch_results)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fae44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, ndcg_score\n",
    "import time\n",
    "\n",
    "def evaluate_service(service, test_users, test_items, test_ratings, sample_size=5000):\n",
    "    \"\"\"Comprehensive evaluation\"\"\"\n",
    "    indices = np.random.choice(len(test_users), min(sample_size, len(test_users)), replace=False)\n",
    "    \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    latencies = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        u, i, r = test_users[idx], test_items[idx], test_ratings[idx]\n",
    "        \n",
    "        start = time.time()\n",
    "        pred = service.predict_score(u, i)\n",
    "        latencies.append(time.time() - start)\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        actuals.append(r)\n",
    "    \n",
    "    auc = roc_auc_score(actuals, predictions)\n",
    "    avg_latency = np.mean(latencies)\n",
    "    p95_latency = np.percentile(latencies, 95)\n",
    "    \n",
    "    print(f\"\\nService Performance:\")\n",
    "    print(f\"  AUC: {auc:.4f}\")\n",
    "    print(f\"  Avg Latency: {avg_latency*1000:.2f}ms\")\n",
    "    print(f\"  P95 Latency: {p95_latency*1000:.2f}ms\")\n",
    "    \n",
    "    return {'auc': auc, 'avg_latency': avg_latency, 'p95_latency': p95_latency}\n",
    "\n",
    "# evaluate\n",
    "metrics = evaluate_service(service, test_users, test_items, test_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7845ec1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44504447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85b1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2259380f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816f179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10fb376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad8ebe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
